# Pipeline de An√°lisis de Deformaci√≥n Terrestre con IA

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/status-activo-brightgreen.svg)

Este repositorio contiene el pipeline de an√°lisis de im√°genes satelitales desarrollado por el equipo **UNICEN Exactas** como parte de nuestra soluci√≥n para el **Hackathon NASA Space Apps Challenge**. El proyecto se enfoca en la automatizaci√≥n del procesamiento de datos de deformaci√≥n del terreno (InSAR) para identificar patrones de comportamiento mediante modelos de Machine Learning no supervisado y predecir tendencias a futuro.

## üõ∞Ô∏è Introducci√≥n

El objetivo de este pipeline es procesar grandes vol√∫menes de datos satelitales de radar de apertura sint√©tica (SAR) para monitorear deformaciones en la superficie terrestre, como hundimientos o levantamientos. El sistema automatiza desde la descarga de los datos crudos hasta la generaci√≥n de predicciones, pasando por un riguroso control de calidad y un an√°lisis de clustering para detectar zonas con comportamientos an√≥malos.

---

## ‚ú® Caracter√≠sticas Principales

-   **Adquisici√≥n Automatizada:** Descarga datos InSAR directamente desde el Alaska Satellite Facility (ASF) para cualquier regi√≥n del mundo.
-   **Procesamiento de Datos:** Convierte eficientemente los archivos GeoTIFF crudos a un formato H5 optimizado para an√°lisis.
-   **Control de Calidad:** Genera una m√°scara de coherencia para filtrar p√≠xeles de baja calidad, asegurando la fiabilidad de los resultados.
-   **Clustering Inteligente:** Utiliza K-Means para series temporales (`tslearn`) para agrupar p√≠xeles con patrones de deformaci√≥n similares.
-   **Interpretaci√≥n y Predicci√≥n:** Analiza la velocidad de deformaci√≥n de cada cl√∫ster y utiliza el modelo Prophet de Meta para predecir su comportamiento a largo plazo.
-   **Alta Configurabilidad:** Dise√±ado para ser f√°cilmente adaptable a nuevas regiones de estudio modificando un √∫nico archivo de configuraci√≥n.

---

## üìÇ Estructura del Repositorio

El proyecto est√° organizado de manera modular para facilitar su mantenimiento y escalabilidad.

|-- üìÇ data/                  # Carpeta para los datos (no incluida en el repo)
|   |-- üìÇ insar_raw_downloads/ # Datos crudos descargados
|   |-- mendoza_timeseries_dataset.h5 # Dataset procesado
|
|-- üìÇ output/                # Carpeta para los resultados generados
|   |-- üìÇ analisis_clustering/ # Gr√°ficos y reportes de la interpretaci√≥n
|   |-- üìÇ quality_mask/      # M√°scara de calidad generada
|   |-- üìÇ saved_model/       # Modelo K-Means entrenado y etiquetas
|
|-- üìÇ src/                   # C√≥digo fuente del proyecto
|   |-- üìÇ logic/             # Clases que encapsulan la l√≥gica principal
|   |   |-- data_manager.py     # L√≥gica de descarga y procesamiento H5
|   |   |-- preprocessor.py     # L√≥gica del control de calidad
|   |   |-- optimizer.py        # L√≥gica del m√©todo del codo
|   |   |-- classifier.py       # L√≥gica del clustering K-Means
|   |   |-- interpreter.py      # L√≥gica de an√°lisis y predicci√≥n
|   |
|   |-- config.py             # ¬°El archivo m√°s importante para la configuraci√≥n!
|   |-- run_*.py              # Scripts ejecutables para cada paso del pipeline
|
|-- README.md                 # Este archivo
|-- requirements.txt          # Dependencias de Python

---

## üîß C√≥mo Adaptarlo a una Nueva Regi√≥n

Este pipeline fue dise√±ado para ser flexible. Para analizar una nueva √°rea geogr√°fica, solo necesitas modificar el archivo `src/config.py`.

**Los par√°metros clave a cambiar son:**

1.  **`ASF_SEARCH_PARAMS`**:
    -   `'intersectsWith'`: Este es el m√°s importante. Debes reemplazar el pol√≠gono de Mendoza por el **pol√≠gono de tu nueva √°rea de inter√©s**. Puedes obtenerlo f√°cilmente desde la [interfaz de b√∫squeda de ASF](https://search.asf.alaska.edu/).

2.  **`DATASET_H5_PATH`**:
    -   Cambia el nombre del archivo de salida (ej: `"neuquen_timeseries_dataset.h5"`) para mantener la organizaci√≥n.

3.  **`MAP_HEIGHT` y `MAP_WIDTH`**:
    -   Estos valores dependen de las dimensiones de los archivos GeoTIFF de tu nueva regi√≥n. Deber√°s actualizarlos despu√©s del primer procesamiento para que los mapas se generen correctamente. El script de preprocesamiento (`run_preprocessing.py`) te informar√° sobre las dimensiones comunes de tus im√°genes.

4.  **Par√°metros del Modelo (Opcional)**:
    -   Despu√©s de ejecutar el m√©todo del codo (`run_find_optimal_k.py`) para tu nueva regi√≥n, actualiza `K_CLUSTERS` con el nuevo valor √≥ptimo.
    -   Puedes ajustar `CLUSTERS_TO_UNIFY` para que la predicci√≥n se enfoque en los nuevos cl√∫steres de inter√©s.

---

## ‚öôÔ∏è Gu√≠a de Ejecuci√≥n del Pipeline

Sigue estos pasos en orden desde la terminal, ubic√°ndote en la carpeta ra√≠z del proyecto.

### 1. Preparaci√≥n del Entorno
Primero, clona el repositorio y crea un entorno virtual (recomendado).

```bash
# Clona el repositorio
git clone <url-del-repositorio>
cd <nombre-del-repositorio>

# Crea y activa un entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instala las dependencias
pip install -r requirements.txt


# Ejecuta los siguientes scripts en orden. Cada uno representa una fase del an√°lisis.
# PASO 0: Descarga los datos crudos y los convierte a H5
python src/run_data_preparation.py

# PASO 1: Crea la m√°scara de coherencia para el control de calidad
python src/run_preprocessing.py

# PASO 2: Ejecuta el m√©todo del codo para encontrar el 'k' √≥ptimo
# (Recuerda actualizar config.py con el 'k' resultante)
python src/run_find_optimal_k.py

# PASO 3: Entrena el modelo de clustering y clasifica los p√≠xeles
python src/run_classification.py

# PASO 4: Interpreta los resultados y genera la predicci√≥n a futuro
python src/run_interpretation.py

Al finalizar, todos los resultados (gr√°ficos, reportes, modelo y datos procesados) estar√°n disponibles en la carpeta output/.

üìÑ Licencia
Este proyecto se distribuye bajo la Licencia MIT. Consulta el archivo LICENSE para m√°s detalles.

üë• Autores
Equipo: UNICEN Exactas

Evento: Hackathon NASA Space Apps Challenge
